{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ekphrasis in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (0.5.1)\r\n",
      "Requirement already satisfied: tqdm in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from ekphrasis) (4.43.0)\r\n",
      "Requirement already satisfied: matplotlib in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from ekphrasis) (3.2.0)\r\n",
      "Requirement already satisfied: nltk in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from ekphrasis) (3.4.5)\r\n",
      "Requirement already satisfied: ftfy in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from ekphrasis) (5.7)\r\n",
      "Requirement already satisfied: colorama in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from ekphrasis) (0.4.3)\r\n",
      "Requirement already satisfied: ujson in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from ekphrasis) (2.0.3)\r\n",
      "Requirement already satisfied: numpy in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from ekphrasis) (1.18.1)\r\n",
      "Requirement already satisfied: termcolor in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from ekphrasis) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from matplotlib->ekphrasis) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from matplotlib->ekphrasis) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from matplotlib->ekphrasis) (2.8.1)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from matplotlib->ekphrasis) (2.4.6)\r\n",
      "Requirement already satisfied: six in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from nltk->ekphrasis) (1.14.0)\r\n",
      "Requirement already satisfied: wcwidth in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from ftfy->ekphrasis) (0.1.8)\r\n",
      "Requirement already satisfied: setuptools in /home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (45.2.0.post20200210)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zqxh49/anaconda3/envs/smm4h/lib/python3.8/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"elongated\", \"repeated\",\n",
    "        'emphasis'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=False).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = ' '.join(text_processor.pre_process_doc(tweet)) # apply function normalize, segment, correct etc...\n",
    "    tweet = ''.join(i for i in tweet if ord(i) < 128) # remove non-ASCII chars from data\n",
    "    tweet = ' '.join(w for w in tknzr.tokenize(tweet)) # tokenize to strip of empty spaces\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/interim/train.csv', header=None)\n",
    "df_val = pd.read_csv('../data/interim/valid.csv', header=None)\n",
    "df_test = pd.read_csv('../data/interim/test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of lines in train data 55419\n",
      "No of lines in train data 13853\n",
      "No of lines in train data 9622\n"
     ]
    }
   ],
   "source": [
    "# print number of lines in data before preprocessing\n",
    "print(f'No of lines in train data {df_train.shape[0]}')\n",
    "print(f'No of lines in train data {df_val.shape[0]}')\n",
    "print(f'No of lines in train data {df_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess tweet\n",
    "df_train['preprocessed_tweet'] = df_train.iloc[:,1].apply(preprocess)\n",
    "df_val['preprocessed_tweet'] = df_val.iloc[:,1].apply(preprocess)\n",
    "df_test['preprocessed_tweet'] = df_test.iloc[:,1].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with empty preprocessed text column\n",
    "df_train = df_train[df_train.preprocessed_tweet != '']\n",
    "df_val = df_val[df_val.preprocessed_tweet != '']\n",
    "df_test = df_test[df_test.preprocessed_tweet != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of lines in train data 55255\n",
      "No of lines in train data 13816\n",
      "No of lines in train data 9622\n"
     ]
    }
   ],
   "source": [
    "# print number of lines in data after preprocessing\n",
    "print(f'No of lines in train data {df_train.shape[0]}')\n",
    "print(f'No of lines in train data {df_val.shape[0]}')\n",
    "print(f'No of lines in train data {df_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export data with necessary columns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../data/processed/train.csv', columns = [0, 'preprocessed_tweet'], index=False, header=False)\n",
    "df_val.to_csv('../data/processed/valid.csv', columns = [0, 'preprocessed_tweet'], index=False, header=False)\n",
    "df_test.to_csv('../data/processed/test.csv', columns = [0, 'preprocessed_tweet'], index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
