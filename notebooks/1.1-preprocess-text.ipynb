{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you Jonathan & Jennifer for this wonderful course! The Professor is brilliant and charming, Jennifer the same (and such an amazing smile!). retired, For anyone who plans to visit Chicago sometime, check out the Improvised Shakespeare show at the iO (formerly Improv Olympic formerly Del Close) Theater. Never been but it's supposed to be a lot of fun; I'll be taking my daughter there later this month. This would be a fun topic for the Shakespeare influences assignment, I think ;-> https://www.facebook.com/iOChicago http://ioimprov.com/\n"
     ]
    }
   ],
   "source": [
    "# Exemplo of a comment from the database:\n",
    "\n",
    "text = \"Thank you Jonathan & Jennifer for this wonderful course! The Professor is brilliant and charming, Jennifer the same (and such an amazing smile!). retired, For anyone who plans to visit Chicago sometime, check out the Improvised Shakespeare show at the iO (formerly Improv Olympic formerly Del Close) Theater. Never been but it's supposed to be a lot of fun; I'll be taking my daughter there later this month. This would be a fun topic for the Shakespeare influences assignment, I think ;-> https://www.facebook.com/iOChicago http://ioimprov.com/\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a text with contraction'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "replace_contractions(\"this's a text with contraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you Jonathan & Jennifer for this wonderful course! The Professor is brilliant and charming, Jennifer the same (and such an amazing smile!). retired, For anyone who plans to visit Chicago sometime, check out the Improvised Shakespeare show at the iO (formerly Improv Olympic formerly Del Close) Theater. Never been but it is supposed to be a lot of fun; I will be taking my daughter there later this month. This would be a fun topic for the Shakespeare influences assignment, I think ;-> https://www.facebook.com/iOChicago http://ioimprov.com/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean = replace_contractions(text)\n",
    "text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_urls(text):\n",
    "    URLless_string = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', 'URL', text)\n",
    "    return URLless_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you Jonathan & Jennifer for this wonderful course! The Professor is brilliant and charming, Jennifer the same (and such an amazing smile!). retired, For anyone who plans to visit Chicago sometime, check out the Improvised Shakespeare show at the iO (formerly Improv Olympic formerly Del Close) Theater. Never been but it is supposed to be a lot of fun; I will be taking my daughter there later this month. This would be a fun topic for the Shakespeare influences assignment, I think ;-> URL URL\n"
     ]
    }
   ],
   "source": [
    "#Usage\n",
    "text_clean = clean_urls(text_clean)\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Special Characters and Separating Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you Jonathan  &  Jennifer for this wonderful course !  The Professor is brilliant and charming ,  Jennifer the same  ( and such an amazing smile !  )  .  retired ,  For anyone who plans to visit Chicago sometime ,  check out the Improvised Shakespeare show at the iO  ( formerly Improv Olympic formerly Del Close )  Theater .  Never been but it is supposed to be a lot of fun ;  I will be taking my daughter there later this month .  This would be a fun topic for the Shakespeare influences assignment ,  I think  ;  -  >  URL URL'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Usage\n",
    "text_clean = clean_special_chars(text_clean)\n",
    "text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d', x)):\n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]{4}', '####', x)\n",
    "        x = re.sub('[0-9]{3}', '###', x)\n",
    "        x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demonstration with ## number ##### ##### test'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "clean_numbers('demonstration with 99 number 22222 2222222233 test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def removing_stopwords(text):\n",
    "    text_clean = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    #return ' '.join(text_clean)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'stopwords']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "removing_stopwords(\"this is a text with stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank',\n",
       " 'Jonathan',\n",
       " '&',\n",
       " 'Jennifer',\n",
       " 'wonderful',\n",
       " 'course',\n",
       " '!',\n",
       " 'Professor',\n",
       " 'brilliant',\n",
       " 'charming',\n",
       " ',',\n",
       " 'Jennifer',\n",
       " '(',\n",
       " 'amazing',\n",
       " 'smile',\n",
       " '!',\n",
       " ')',\n",
       " '.',\n",
       " 'retired',\n",
       " ',',\n",
       " 'anyone',\n",
       " 'plans',\n",
       " 'visit',\n",
       " 'Chicago',\n",
       " 'sometime',\n",
       " ',',\n",
       " 'check',\n",
       " 'Improvised',\n",
       " 'Shakespeare',\n",
       " 'show',\n",
       " 'iO',\n",
       " '(',\n",
       " 'formerly',\n",
       " 'Improv',\n",
       " 'Olympic',\n",
       " 'formerly',\n",
       " 'Del',\n",
       " 'Close',\n",
       " ')',\n",
       " 'Theater',\n",
       " '.',\n",
       " 'Never',\n",
       " 'supposed',\n",
       " 'lot',\n",
       " 'fun',\n",
       " ';',\n",
       " 'taking',\n",
       " 'daughter',\n",
       " 'later',\n",
       " 'month',\n",
       " '.',\n",
       " 'would',\n",
       " 'fun',\n",
       " 'topic',\n",
       " 'Shakespeare',\n",
       " 'influences',\n",
       " 'assignment',\n",
       " ',',\n",
       " 'think',\n",
       " ';',\n",
       " '-',\n",
       " '>',\n",
       " 'URL',\n",
       " 'URL']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean = removing_stopwords(text_clean)\n",
    "text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foot'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"feet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It will work for a list of strings\n",
    "def lemmatizer_text(text):\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        new_text.append(lemma)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank',\n",
       " 'Jonathan',\n",
       " '&',\n",
       " 'Jennifer',\n",
       " 'wonderful',\n",
       " 'course',\n",
       " '!',\n",
       " 'Professor',\n",
       " 'brilliant',\n",
       " 'charming',\n",
       " ',',\n",
       " 'Jennifer',\n",
       " '(',\n",
       " 'amazing',\n",
       " 'smile',\n",
       " '!',\n",
       " ')',\n",
       " '.',\n",
       " 'retired',\n",
       " ',',\n",
       " 'anyone',\n",
       " 'plan',\n",
       " 'visit',\n",
       " 'Chicago',\n",
       " 'sometime',\n",
       " ',',\n",
       " 'check',\n",
       " 'Improvised',\n",
       " 'Shakespeare',\n",
       " 'show',\n",
       " 'iO',\n",
       " '(',\n",
       " 'formerly',\n",
       " 'Improv',\n",
       " 'Olympic',\n",
       " 'formerly',\n",
       " 'Del',\n",
       " 'Close',\n",
       " ')',\n",
       " 'Theater',\n",
       " '.',\n",
       " 'Never',\n",
       " 'supposed',\n",
       " 'lot',\n",
       " 'fun',\n",
       " ';',\n",
       " 'taking',\n",
       " 'daughter',\n",
       " 'later',\n",
       " 'month',\n",
       " '.',\n",
       " 'would',\n",
       " 'fun',\n",
       " 'topic',\n",
       " 'Shakespeare',\n",
       " 'influence',\n",
       " 'assignment',\n",
       " ',',\n",
       " 'think',\n",
       " ';',\n",
       " '-',\n",
       " '>',\n",
       " 'URL',\n",
       " 'URL']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_text(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "* Perhaps Stemming would sound better for us since text might have many verbs\n",
    "* Perhaps none of them;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It will work for a list of strings\n",
    "def stemmer_text(text):\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        stem = stemmer.stem(word)\n",
    "        new_text.append(stem)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank',\n",
       " 'jonathan',\n",
       " '&',\n",
       " 'jennif',\n",
       " 'wonder',\n",
       " 'cours',\n",
       " '!',\n",
       " 'professor',\n",
       " 'brilliant',\n",
       " 'charm',\n",
       " ',',\n",
       " 'jennif',\n",
       " '(',\n",
       " 'amaz',\n",
       " 'smile',\n",
       " '!',\n",
       " ')',\n",
       " '.',\n",
       " 'retir',\n",
       " ',',\n",
       " 'anyon',\n",
       " 'plan',\n",
       " 'visit',\n",
       " 'chicago',\n",
       " 'sometim',\n",
       " ',',\n",
       " 'check',\n",
       " 'improvis',\n",
       " 'shakespear',\n",
       " 'show',\n",
       " 'iO',\n",
       " '(',\n",
       " 'formerli',\n",
       " 'improv',\n",
       " 'olymp',\n",
       " 'formerli',\n",
       " 'del',\n",
       " 'close',\n",
       " ')',\n",
       " 'theater',\n",
       " '.',\n",
       " 'never',\n",
       " 'suppos',\n",
       " 'lot',\n",
       " 'fun',\n",
       " ';',\n",
       " 'take',\n",
       " 'daughter',\n",
       " 'later',\n",
       " 'month',\n",
       " '.',\n",
       " 'would',\n",
       " 'fun',\n",
       " 'topic',\n",
       " 'shakespear',\n",
       " 'influenc',\n",
       " 'assign',\n",
       " ',',\n",
       " 'think',\n",
       " ';',\n",
       " '-',\n",
       " '>',\n",
       " 'url',\n",
       " 'url']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_text(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Misspells:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: use the following command on your terminal to download the GoogleNews-vectors-negative300\n",
    "\n",
    "conda install gensim\n",
    "wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "Source: [here](https://stackoverflow.com/questions/46433778/import-googlenews-vectors-negative300-bin)\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that the following code is an adaptation of Peter Norvig’s spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b00ed8a595e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import gensim\n",
    "import heapq\n",
    "from operator import itemgetter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin', \n",
    "                                                        binary=True)\n",
    "words = model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab('Exemple avec des mots au hasard ')\n",
    "\n",
    "top_90k_words = dict(heapq.nlargest(90000, vocab.items(), key=itemgetter(1)))\n",
    "\n",
    "pool = Pool(4)\n",
    "corrected_words = pool.map(correction,list(top_90k_words.keys()))\n",
    "\n",
    "for word,corrected_word in zip(top_90k_words,corrected_words):\n",
    "    if word!=corrected_word:\n",
    "        print(word,\":\",corrected_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Or you can check (after tokenization) if each word exists in the nltk corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/dwan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "word_list = brown.words()\n",
    "len(word_list)\n",
    "\n",
    "word_set = set(word_list)\n",
    "\"looked\" in word_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Or just install the following Spellchecker\n",
    "\n",
    "* pip install pyspellchecker\n",
    "* [Link](https://github.com/barrust/pyspellchecker) with more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n",
      "are\n",
      "you\n",
      "doing\n"
     ]
    }
   ],
   "source": [
    "spell = SpellChecker()  # loads default word frequency list\n",
    "words = spell.split_words('waht are you donig?')\n",
    "for w in words:\n",
    "    print(spell.correction(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Tfidf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf = TfidfVectorizer(sublinear_tf=False, min_df=100, norm='None', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "tfidf = TfidfVectorizer(min_df=50, encoding='latin-1', stop_words='english')\n",
    "\n",
    "features = tfidf.fit_transform(train_data['text']).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_data.category_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- all dara\n",
    "\n",
    "print(\"processing the data...\")\n",
    "for index,value in enumerate(x):\n",
    "    #print(\"processing data:\",index)\n",
    "    x[index] = ' '.join([Word(word).lemmatize() for word in clean_str(value).split()])\n",
    "\n",
    "vect = TfidfVectorizer(stop_words='english',min_df=500)\n",
    "X = vect.fit_transform(x)\n",
    "Y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- per categories\n",
    "\n",
    "\n",
    "\n",
    "N = 5  # We are going to look for top 5 categories /words\n",
    "#For each category, find words that are highly corelated to it\n",
    "for Category, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == category_id)                   # Do chi2 analyses of all items in this category\n",
    "  indices = np.argsort(features_chi2[0])                                  # Sorts the indices of features_chi2[0] - the chi-squared stats of each feature\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]            # Converts indices to feature names ( in increasing order of chi-squared stat values)\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]         # List of single word features ( in increasing order of chi-squared stat values)\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]          # List for two-word features ( in increasing order of chi-squared stat values)\n",
    "  print(\"# '{}':\".format(Category))\n",
    "  print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:]))) # Print 5 unigrams with highest Chi squared stat\n",
    "  print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:]))) # Print 5 bigrams with highest Chi squared stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
